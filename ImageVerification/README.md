Following a Stanford tutorial on data generation, I loaded data in partitions to most effectively access the image data without depleting my computer memory.  After reading a few papers about image verification, I decided to try a network with triplet loss in which I mapped the distances between A and B and A and C, anchor with positive and anchor with negative respectively, to create a lower-dimensional classification problem.  Then, following a suggestion from the tutorial Imputation and CNN, I decided also to use a pre-trained model.  The one that seemed easiest to implement and most suited to my needs was TensorFlowâ€™s NasNetMobile.  (I also tried MobileNet, a smaller pre-trained model base, and did not achieve as good results as with NasNet.) I froze the weights of the pre-trained model and just added embedding, concatenation, and dense layers on top of the pre-trained model.  Originally, I had more than 2 million trainable parameters, which led to some overfitting issues and very long training periods, and chose to freeze and tie more weights so that only 170,000 or so parameters were trainable.  Overall, the final model has essentially three sub-models: the pre-trained model, the embedding/PCA, and the Classification model.  I also considered, though eventually phased out, a Siamese network.
